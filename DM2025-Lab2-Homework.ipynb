{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:張凱鈞\n",
    "\n",
    "Student ID:M11224005\n",
    "\n",
    "GitHub ID:arrowtime0913\n",
    "\n",
    "Kaggle name:Changtim\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)\n",
    "![pic_ranking.png.PNG](./pics/pic_ranking.png.PNG)\n",
    "# mark:此紀錄是在上傳之後當下的排名，後來排名已有所變動(分數未有變動)，在此敘明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "*   *資料清洗*: `*包含一律轉小寫、去除網址、標記、空格(此過程在特徵工程之後)`\n",
    "*   *資料讀取*: `*從競賽提供的json資料中讀出我們要的資料`\n",
    "*   *資料合併*: `*把競賽提供的三個檔案的資料，整理成我要的訓練資料集train_df和測試資料集test_df`\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "*   *合併hastag*: `*將hastag的內容合併進text裡，並留一個\" \"，讓模型更好了解`\n",
    "*   *情緒標籤轉換*: `*使用labelencoder，把各個情緒標籤轉成模型看得懂的數字標籤`\n",
    "*   *tokenizer*: `*使用pre-trained的tokenizer，將文本轉成詞索引和注意力遮罩(simpletransformer這個函式庫在背後已經幫我們做好了)`\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "*   *roberta-large*: `*挑選roberta-large作為第一模型進行訓練`\n",
    "*   *bert-large-uncased*: `*挑選bert-large-uncased作為第二模型進行訓練`\n",
    "*   *兩模型一起決定(模型融合)*: `*讓兩個模型一起對測試資料集進行推論`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "第一次是用，高宏宇老師在NLP課程中提到的roberta(roberta的訓練資料大多都是來自網路的文章(文章)，跟本競賽所用的資料集向性蠻相符的)\n",
    "(另外，encoder架構的transformer更適合做分類任務)，基於以上的分析，所以我選擇了roberta-base作為我第一次訓練用的模型\n",
    "但因為希望先快速有一個初步的結果，確定我的資料前處理和特徵工程沒有問題。所以只先用了基礎款，在確定有初步的結果後:\n",
    "![./pics/DM_lab2.png](./pics/DM_lab2.png)\n",
    "我將roberta換成large版，並且把資料清洗中去除標點符號拿掉(有一些標點符號在判斷情緒上可以提供內容 如:) )，結果如下\n",
    "![./pics/DM_lab2_two.png](./pics/DM_lab2_two.png)\n",
    "接著為了進一步分數，我希望在評分的階段能由同為encoder的bert一起共同評分(bert的訓練資料集是來自twitter的語料與本競賽中網路論壇的文本相性更相符)，結果如下:\n",
    "![./pics/pic_ranking.png.png](./pics/pic_ranking.png.png)\n",
    "進一步的，我反覆嘗試將兩模型的權重調得非一半一半(一版是roberta較大 一版是bert較大)\n",
    "![./pics/DM_lab2_four.png](./pics/DM_lab2_four.png)\n",
    "### 2.2 Mention Insights You Gained\n",
    "[Content for Insights]\n",
    "<建議點開來檢視比較好閱讀>\n",
    "在程式的完成的過程中，當然不是像2.1的所描述的這麼簡單\n",
    "從觀察資料決定選什麼模型、做怎樣的清洗、如何利用hastag等等的都是寶貴的實作過程\n",
    "另外，更重要的在於，當結果初成的時候該怎麼把模型的結果調到更好\n",
    "2.1紀錄的只有成功的，但當然也有試了之後的廢案\n",
    "例如:把hastag加進文本之後做額外的標註\n",
    "以及2.1提到的(圖片中只有兩個案例)，調整兩模型權重的數次嘗試(圖片(four.png)只是擷取一部分的過程)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import os\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#把要做的清洗寫成一個函式，視情況可改這一part\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    #text = text.translate(str.maketrans('', '', string.punctuation)) #有一些表情符號的使用，如:)也可以代表情緒，這一項待考慮\n",
    "    return text\n",
    "    \n",
    "#抓路徑\n",
    "base_path = ''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    if 'final_posts.json' in filenames:\n",
    "        base_path = dirname\n",
    "        break\n",
    "print(f\"Data directory:{base_path}\")\n",
    "\n",
    "#讀CSV\n",
    "print(\"讀CSV\")\n",
    "ident_csv = pd.read_csv(os.path.join(base_path,'data_identification.csv'))\n",
    "from_emotion_csv = pd.read_csv(os.path.join(base_path,'emotion.csv'))\n",
    "\n",
    "#處理final_posts.json\n",
    "print(\"在處理final_posts.json\")\n",
    "with open(os.path.join(base_path, 'final_posts.json'), 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "#拆解final_posts.json，這個資料的結構比較特別(巢狀)\n",
    "parsedisallyouneed = []\n",
    "for item in raw_data:\n",
    "    contentsss = item['root']['_source']['post']\n",
    "    parsedisallyouneed.append(contentsss)\n",
    "from_post_json = pd.DataFrame(parsedisallyouneed)\n",
    "\n",
    "#統一欄位名稱\n",
    "if 'post_id' in from_post_json.columns:\n",
    "    from_post_json = from_post_json.rename(columns={'post_id': 'id'})\n",
    "\n",
    "#資料合併\n",
    "print(\"上面都還沒有問題，正在合併\")\n",
    "full_concentrate = pd.merge(ident_csv, from_post_json, how='left', on='id')\n",
    "\n",
    "#切割Train/Test使用split來篩選，做出最後要拿來用的訓練資料及和測試資料集\n",
    "train_df = full_concentrate[full_concentrate['split'] == 'train'].copy()\n",
    "test_df = full_concentrate[full_concentrate['split'] == 'test'].copy()\n",
    "\n",
    "#把測試資料的emotion貼回去\n",
    "train_df = pd.merge(train_df, from_emotion_csv, how='left', on='id')\n",
    "\n",
    "#輸出結果\n",
    "print(f\"訓練資料大小:{train_df.shape}\")\n",
    "print(f\"測試資料大小:{test_df.shape}\")\n",
    "print(\"\\n訓練資料前3筆\")\n",
    "display(train_df.head(20))\n",
    "print(\"\\n測試資料前3筆\")\n",
    "display(test_df.head(20))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#將Hashtags合併到Text裡面\n",
    "train_df['tags_str'] = train_df['hashtags'].str.join(' ')\n",
    "test_df['tags_str'] = test_df['hashtags'].str.join(' ')\n",
    "\n",
    "#有些沒hashtag出來會變NaN，把NaN變成空字串，不然等一下相加會報錯\n",
    "train_df['tags_str'] = train_df['tags_str'].fillna('')\n",
    "test_df['tags_str'] = test_df['tags_str'].fillna('')\n",
    "\n",
    "#直接將兩欄文字相加\n",
    "train_df['raw_text'] = train_df['text'] + \" \" + train_df['tags_str']\n",
    "test_df['raw_text'] = test_df['text'] + \" \" + test_df['tags_str']\n",
    "\n",
    "print(\"合併後的樣子:\")\n",
    "print(train_df['raw_text'].iloc[0])\n",
    "\n",
    "#清洗(按照目前的規劃hastag併進去再清洗比較合理，但因為作業格式要求前處理和特徵工程的code cell要分開(先前處理後特徵工程))，所以我把清洗資料的函式寫在前面前處理的部分，在需要用的時候再使用)\n",
    "train_df['clean_text'] = train_df['raw_text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['raw_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"hastag合併和清洗已完成，抽範例來看\\n\")\n",
    "display(train_df[['text', 'clean_text', 'emotion']].head())\n",
    "\n",
    "\n",
    "#將情緒標籤轉成數字\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_df['labels'] = label_encoder.fit_transform(train_df['emotion'])\n",
    "train_data = train_df[['clean_text', 'labels']].rename(columns={'clean_text': 'text'})\n",
    "#看模型怎麼轉\n",
    "print(\"表:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")\n",
    "\n",
    "print(\"\\n抽檔案出來，確定有無問題\")\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y pyarrow\n",
    "#!pip install pyarrow==14.0.1\n",
    "#!pip install protobuf==3.20.3\n",
    "#!pip install simpletransformers\n",
    "#第一次執行時執行以上pip套件下載(要反覆執行兩次 第一次載完報錯後重啟環境 再載第二次就可以執行了)\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "#紀錄訓練進度\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "#超參數\n",
    "model_args = ClassificationArgs()\n",
    "model_args.num_train_epochs = 3           \n",
    "#model_args.train_batch_size = 32\n",
    "model_args.train_batch_size = 16 #調到large要用小一點的batchsize\n",
    "model_args.learning_rate = 4e-5           \n",
    "model_args.max_seq_length = 128           \n",
    "model_args.overwrite_output_dir = True    \n",
    "model_args.save_steps = -1                \n",
    "model_args.save_model_every_epoch = False\n",
    "\n",
    "\n",
    "#model = ClassificationModel(\"roberta\",\"roberta-base\",num_labels=6,args=model_args,use_cuda=True)\n",
    "model = ClassificationModel(\"roberta\",\"roberta-large\",num_labels=6,args=model_args,use_cuda=True)\n",
    "model.train_model(train_data)\n",
    "\n",
    "print(\"roberta(large版)訓練結束\")\n",
    "\n",
    "#不加這個Kaggle那邊測試資料集會跑不動(測試資料的問題)\n",
    "model.args.use_multiprocessing = False\n",
    "model.args.use_multiprocessing_for_evaluation = False\n",
    "\n",
    "model.args.eval_batch_size = 64\n",
    "to_predict = test_df['clean_text'].tolist()\n",
    "\n",
    "print(f\"測試資料長度:{len(to_predict)}\")\n",
    "\n",
    "predictions, raw_outputs = model.predict(to_predict)\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_df['id'],'emotion': predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"成功生成結果檔\\n\")\n",
    "\n",
    "print(\"稍微看一下結果，有沒有train壞\")\n",
    "print(submission['emotion'].value_counts())\n",
    "print(\"檢查格式:\")\n",
    "display(submission.head())\n",
    "\n",
    "roberta_storage = raw_outputs\n",
    "\n",
    "#刪模型沒刪記憶體會炸裂\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "#超參數\n",
    "models_args = ClassificationArgs()\n",
    "models_args.use_multiprocessing = False\n",
    "models_args.use_multiprocessing_for_evaluation = False\n",
    "models_args.process_count = 1\n",
    "models_args.dataloader_num_workers = 0\n",
    "models_args.num_train_epochs = 3\n",
    "models_args.train_batch_size = 16\n",
    "models_args.learning_rate = 2e-5\n",
    "models_args.max_seq_length = 128\n",
    "models_args.overwrite_output_dir = True\n",
    "models_args.save_steps = -1\n",
    "models_args.save_model_every_epoch = False\n",
    "models_args.no_cache = True\n",
    "models_args.tensorboard_dir = None\n",
    "models_args.reprocess_input_data = True \n",
    "\n",
    "model_bert = ClassificationModel(\"bert\", \"bert-large-uncased\", num_labels=6, args=models_args, use_cuda=True)\n",
    "\n",
    "model_bert.train_model(train_data)\n",
    "print(\"bert(large版)訓練結束\")\n",
    "\n",
    "\n",
    "model_bert.args.eval_batch_size = 64\n",
    "to_predict = test_df['clean_text'].tolist()\n",
    "\n",
    "print(f\"測試資料長度:{len(to_predict)}\")\n",
    "\n",
    "predictionss, raw_outputss = model_bert.predict(to_predict)\n",
    "\n",
    "predicted_labelss = label_encoder.inverse_transform(predictionss)\n",
    "\n",
    "submissions = pd.DataFrame({'id': test_df['id'],'emotion': predicted_labelss})\n",
    "submissions.to_csv('submission2.csv', index=False)\n",
    "\n",
    "print(\"成功生成結果檔\\n\")\n",
    "\n",
    "print(\"稍微看一下結果，有沒有train壞\")\n",
    "print(submission['emotion'].value_counts())\n",
    "\n",
    "print(\"檢查格式:\")\n",
    "display(submission.head())\n",
    "\n",
    "bert_storage = raw_outputss\n",
    "\n",
    "\n",
    "final_raw_outputs = (roberta_storage + bert_storage)/2\n",
    "#final_raw_outputs = (0.6*roberta_storage + 0.4*bert_storage) \n",
    "\n",
    "final_preds = np.argmax(final_raw_outputs, axis=1)\n",
    "\n",
    "#存檔\n",
    "final_labels = label_encoder.inverse_transform(final_preds)\n",
    "submissionss = pd.DataFrame({'id': test_df['id'], 'emotion': final_labels})\n",
    "submissionss.to_csv('submission_ensemble_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
